{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_snipet_metadataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOI+F2c3BcBwCQjHo1pWQs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyankaTUI/AudioClassificationWithDeepLearningAnalysis/blob/master/dataset/data_processing_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PriyankaTUI/AudioClassificationWithDeepLearningAnalysis.git\n",
        "%cd AudioClassificationWithDeepLearningAnalysis/dataset\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUQmWZCkw_vZ",
        "outputId": "c5c9c2bf-2a93-47b2-8933-179b7e20b2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AudioClassificationWithDeepLearningAnalysis'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 196 (delta 40), reused 29 (delta 23), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (196/196), 44.37 MiB | 14.75 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "/content/AudioClassificationWithDeepLearningAnalysis/dataset/AudioClassificationWithDeepLearningAnalysis/dataset\n",
            "/content/AudioClassificationWithDeepLearningAnalysis/dataset/AudioClassificationWithDeepLearningAnalysis/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We have two different approaches for pre data processing. Based on time management and memory management, we will compare both approaches below."
      ],
      "metadata": {
        "id": "IIpmdBqrwf9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Fist data processing approach\n",
        "\n",
        "\n",
        "**With this strategy, all data will be processed beforehand and stored locally for use during training.**\n"
      ],
      "metadata": {
        "id": "sQbll3kAxNUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P616LuuXVjCI"
      },
      "outputs": [],
      "source": [
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "import torchaudio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler\n",
        "from memory_profiler import profile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlvOCyXsMICE",
        "outputId": "bbb74ced-0777-4650-869a-8eb1530f78f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=867b0d2988ecb256bdb640873fc7b814757cacbaecf4d170f562d27f14b2ec44\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.60.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "id": "0TYvpIU-Rjjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file mprun_data_processing.py\n",
        "\n",
        "\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "import torchaudio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def label_to_index(labels, label):\n",
        "    # Return the position of the word in labels\n",
        "    return torch.tensor(labels.index(label))\n",
        "\n",
        "def index_to_label(labels, index):\n",
        "    # Return the word corresponding to the index in labels\n",
        "    # This is the inverse of label_to_index\n",
        "    return labels[index]\n",
        "\n",
        "    \n",
        "def load_and_preprocess_speech_command_dataset(random_targets, digits):\n",
        "    tensors = []\n",
        "    targets = []\n",
        "\n",
        "    #old classes storage\n",
        "    old_class_tensors = []\n",
        "    old_class_targets = []\n",
        "    \n",
        "    labels = digits + random_targets\n",
        "    # print(f\"Novel classes: {random_targets}\")\n",
        "    # print(f\"Old classes: {digits}\")\n",
        "    # print(f\"List of all classes: {labels}\")\n",
        "\n",
        "    ### saving local file for some data to save time for creating and processing new data\n",
        "    ### while continuously working on project \n",
        "    ### we can delete local files and always create new data\n",
        "    if (os.path.exists(path='data/novel_class_tensors.pt') and \n",
        "        os.path.exists(path='data/novel_class_targets.pt') and \n",
        "        os.path.exists(path='data/old_class_tensors.pt') and \n",
        "        os.path.exists(path='data/old_class_targets.pt')):\n",
        "        \n",
        "        tensors = torch.load('data/novel_class_tensors.pt')\n",
        "        targets = torch.load('data/novel_class_targets.pt')\n",
        "        old_class_tensors = torch.load('data/old_class_tensors.pt')\n",
        "        old_class_targets = torch.load('data/old_class_targets.pt')\n",
        "\n",
        "    else:\n",
        "\n",
        "        #  Loading dataset and custom dataloader\n",
        "        dataset = torchaudio.datasets.SPEECHCOMMANDS('./data/' , url = 'speech_commands_v0.02', folder_in_archive= 'SpeechCommands',  download = True)\n",
        "        #parameters for MFCC transformation\n",
        "        n_fft = 2048\n",
        "        win_length = None\n",
        "        hop_length = 512\n",
        "        n_mels = 256\n",
        "        n_mfcc = 256\n",
        "\n",
        "        for waveform, sample_rate, label, *_ in dataset:\n",
        "            if label in random_targets:\n",
        "                if sample_rate == 16000:\n",
        "                    if waveform.shape == (1, 16000):\n",
        "                        tensors += [torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=32, \n",
        "                                                                melkwargs={\n",
        "                                                                            'n_fft': n_fft,\n",
        "                                                                            'n_mels': n_mels,\n",
        "                                                                            'hop_length': hop_length,\n",
        "                                                                            'mel_scale': 'htk',\n",
        "                                                                            }\n",
        "                                                                            )(waveform)]\n",
        "                        targets += [label_to_index(labels, label)]\n",
        "\n",
        "                if label in digits:\n",
        "                    if sample_rate == 16000:\n",
        "                        if waveform.shape == (1, 16000):\n",
        "                            old_class_tensors += [torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=32, \n",
        "                                                                    melkwargs={\n",
        "                                                                                'n_fft': n_fft,\n",
        "                                                                                'n_mels': n_mels,\n",
        "                                                                                'hop_length': hop_length,\n",
        "                                                                                'mel_scale': 'htk',\n",
        "                                                                                }\n",
        "                                                                                )(waveform)]\n",
        "                            old_class_targets += [label_to_index(labels, label)]\n",
        "\n",
        "        torch.save(tensors, 'data/novel_class_tensors.pt')\n",
        "        torch.save(targets, 'data/novel_class_targets.pt')\n",
        "        torch.save(old_class_tensors, 'data/old_class_tensors.pt')\n",
        "        torch.save(old_class_targets, 'data/old_class_targets.pt')\n",
        "\n",
        "    return {\"tensors\": tensors, \"targets\": targets,\n",
        "            \"old_class_tensors\": old_class_tensors, \n",
        "            \"old_class_targets\": old_class_targets}\n",
        "    \n",
        "\n",
        "\n",
        "class SpeechCommandSubDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,data,labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)    \n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        # print(f\"getting data {idx}\")\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "######starting processing##########\n",
        "def data_preprocessing():\n",
        "  random_targets = ['marvin', 'sheila', 'backward' ]\n",
        "  digits = ['zero','one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
        "  data = load_and_preprocess_speech_command_dataset(random_targets=random_targets, digits=digits)\n",
        "  tensors = data[\"tensors\"]\n",
        "  targets = data[\"targets\"]\n",
        "  old_class_tensors = data[\"old_class_tensors\"]\n",
        "  old_class_targets = data[\"old_class_targets\"]\n",
        "  #taking small amount of sample data for training and testing\n",
        "  random_index = np.random.randint(len(tensors), size=50)\n",
        "  # print(f\"Random index: {random_index}\")\n",
        "  valid_dataset = SpeechCommandSubDataset(data = [tensors[index] for index in random_index], \n",
        "                                          labels = [targets[index] for index in random_index])\n",
        "\n",
        "  traindata, testdata = random_split(valid_dataset, [round(len(valid_dataset)*.6), round(len(valid_dataset)*.4)])\n",
        "  trainloader = DataLoader(traindata, batch_size=10, shuffle=True)\n",
        "  testloader = DataLoader(testdata, batch_size=10, shuffle=True)\n",
        "\n",
        "  # #creating old data loder for measures\n",
        "  # random_index_olddata = np.random.randint(len(old_class_tensors), size=20)\n",
        "  # # print(f\"Random index: {random_index}\")\n",
        "  # old_class_dataset = SpeechCommandSubDataset(data = [old_class_tensors[index] for index in random_index_olddata], \n",
        "  #                                         labels = [old_class_targets[index] for index in random_index_olddata])\n",
        "\n",
        "  # old_class_testloader = DataLoader(old_class_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "  #to check time required for iterating training data\n",
        "  for i, (input, lables) in enumerate(traindata):\n",
        "    print(\"\", end= \"\")\n",
        "  \n",
        "  #to check time required for iterating testing data\n",
        "  for i, (input, lables) in enumerate(testloader):\n",
        "    print(\"\", end= \"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyTN4-hUxmbX",
        "outputId": "cba412ef-30b2-449b-a13f-c03a384cc245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mprun_data_processing_3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from mprun_data_processing import data_preprocessing\n",
        "%mprun -f data_preprocessing data_preprocessing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN3HeElSSfPA",
        "outputId": "8c2cd339-c743-4c08-a0be-43b12b1ab4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CPU times: user 1.51 s, sys: 37.2 ms, total: 1.54 s\n",
            "Wall time: 1.54 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second data processing approach\n",
        "\n",
        "**With this strategy, all data will be processed in form of list and processed only during training.**"
      ],
      "metadata": {
        "id": "W_jO00dAxojM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file mprun_meta_data_processing.py\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "import torchaudio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def label_to_index(labels, label):\n",
        "    # Return the position of the word in labels\n",
        "    return torch.tensor(labels.index(label))\n",
        "\n",
        "def index_to_label(labels, index):\n",
        "    # Return the word corresponding to the index in labels\n",
        "    # This is the inverse of label_to_index\n",
        "    return labels[index]\n",
        "\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None, subset_type : str = None):\n",
        "        digits = ['zero','one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
        "        super().__init__(\"./\", download=True)\n",
        "        n_fft = 2048\n",
        "        win_length = None\n",
        "        hop_length = 512\n",
        "        n_mels = 256\n",
        "        n_mfcc = 256\n",
        "        sampling_rate = 16000\n",
        "        self.transform = torchaudio.transforms.MFCC(sample_rate=sampling_rate, n_mfcc=32, \n",
        "                                                                        melkwargs={\n",
        "                                                                                    'n_fft': n_fft,\n",
        "                                                                                    'n_mels': n_mels,\n",
        "                                                                                    'hop_length': hop_length,\n",
        "                                                                                    'mel_scale': 'htk',\n",
        "                                                                                    }\n",
        "                                                                                    )\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as fileobj:\n",
        "                if subset_type == 'old':\n",
        "                    return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj if line.startswith(tuple(digits))]\n",
        "                elif subset_type == 'novel':\n",
        "                    return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj if not line.startswith(tuple(digits))]\n",
        "                else:\n",
        "                    return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
        "\n",
        "        if subset == \"training\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "\n",
        "            \n",
        "    def __getitem__(self,idx):\n",
        "        waveform, sampling_rate, label, *_ = super().__getitem__(idx)\n",
        "        #returning waveform and it's label\n",
        "        if self.transform is not None:\n",
        "            waveform = self.transform(waveform)\n",
        "        return waveform, label\n",
        "\n",
        "class FewShotBatchSampler(object):\n",
        "\n",
        "    def __init__(self, dataset_targets, N_way, K_shot, include_query=False, shuffle=True, shuffle_once=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            dataset_targets - PyTorch tensor of the labels of the data elements.\n",
        "            N_way - Number of classes to sample per batch.\n",
        "            K_shot - Number of examples to sample per class in the batch.\n",
        "            include_query - If True, returns batch of size N_way*K_shot*2, which\n",
        "                            can be split into support and query set. Simplifies\n",
        "                            the implementation of sampling the same classes but\n",
        "                            distinct examples for support and query set.\n",
        "            shuffle - If True, examples and classes are newly shuffled in each\n",
        "                      iteration (for training)\n",
        "            shuffle_once - If True, examples and classes are shuffled once in\n",
        "                           the beginning, but kept constant across iterations\n",
        "                           (for validation)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dataset_targets = dataset_targets\n",
        "        self.N_way = N_way\n",
        "        self.K_shot = K_shot\n",
        "        self.shuffle = shuffle\n",
        "        self.include_query = include_query\n",
        "        if self.include_query:\n",
        "            self.K_shot *= 2\n",
        "        self.batch_size = self.N_way * self.K_shot  # Number of overall images per batch\n",
        "\n",
        "        # Organize examples by class\n",
        "        self.classes = torch.unique(self.dataset_targets).tolist()\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.indices_per_class = {}\n",
        "        self.batches_per_class = {}  # Number of K-shot batches that each class can provide\n",
        "        for c in self.classes:\n",
        "            self.indices_per_class[c] = torch.where(self.dataset_targets == c)[0]\n",
        "            self.batches_per_class[c] = self.indices_per_class[c].shape[0] // self.K_shot\n",
        "\n",
        "        # Create a list of classes from which we select the N classes per batch\n",
        "        self.iterations = sum(self.batches_per_class.values()) // self.N_way\n",
        "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
        "        if shuffle_once or self.shuffle:\n",
        "            self.shuffle_data()\n",
        "        else:\n",
        "            # For testing, we iterate over classes instead of shuffling them\n",
        "            sort_idxs = [i+p*self.num_classes for i,\n",
        "                         c in enumerate(self.classes) for p in range(self.batches_per_class[c])]\n",
        "            self.class_list = np.array(self.class_list)[np.argsort(sort_idxs)].tolist()\n",
        "\n",
        "    def shuffle_data(self):\n",
        "        # Shuffle the examples per class\n",
        "        for c in self.classes:\n",
        "            perm = torch.randperm(self.indices_per_class[c].shape[0])\n",
        "            self.indices_per_class[c] = self.indices_per_class[c][perm]\n",
        "        # Shuffle the class list from which we sample. Note that this way of shuffling\n",
        "        # does not prevent to choose the same class twice in a batch. However, for\n",
        "        # training and validation, this is not a problem.\n",
        "        random.shuffle(self.class_list)\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Shuffle data\n",
        "        if self.shuffle:\n",
        "            self.shuffle_data()\n",
        "\n",
        "        # Sample few-shot batches\n",
        "        start_index = defaultdict(int)\n",
        "        for it in range(self.iterations):\n",
        "            class_batch = self.class_list[it*self.N_way:(it+1)*self.N_way]  # Select N classes for the batch\n",
        "            index_batch = []\n",
        "            for c in class_batch:  # For each class, select the next K examples and add them to the batch\n",
        "                index_batch.extend(self.indices_per_class[c][start_index[c]:start_index[c]+self.K_shot])\n",
        "                start_index[c] += self.K_shot\n",
        "            if self.include_query:  # If we return support+query set, sort them so that they are easy to split\n",
        "                index_batch = index_batch[::2] + index_batch[1::2]\n",
        "            yield index_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.iterations\n",
        "\n",
        "def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = [item.t() for item in batch]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "    return batch.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "def collate_fn(batch, labels):\n",
        "        tensors, targets = [], []\n",
        "        for waveform, label in batch:\n",
        "                tensors += [torch.squeeze(waveform)]\n",
        "                targets += [label_to_index(labels, label)]\n",
        "                \n",
        "        tensors = torch.unsqueeze(pad_sequence(tensors), 1)\n",
        "        targets = torch.stack(targets)\n",
        "        return tensors, targets\n",
        "\n",
        "def data_processing():\n",
        "  # from torch.utils.data.dataset import Subset\n",
        "  old_train_set = SubsetSC(\"training\", \"old\")\n",
        "  old_test_set = SubsetSC(\"testing\", \"old\")\n",
        "  novel_train_set = SubsetSC(\"training\", \"novel\")\n",
        "  novel_test_set = SubsetSC(\"testing\", \"novel\")\n",
        "\n",
        "  training_data = next(iter(novel_train_set))\n",
        "\n",
        "  #labels would be combination of novel classes and old classes(digits)\n",
        "  targets_list = [os.path.basename(os.path.dirname(novel_train_set._walker[i])) for i in range(len(novel_train_set))]\n",
        "  targets = list(set(targets_list))\n",
        "  digits = ['zero','one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
        "  labels = digits + targets \n",
        "  targets_idx = [label_to_index(targets, i) for i in targets_list]\n",
        "  # targets_idx = [list(targets).index(i) for i in targets_list]\n",
        "  N_WAY = 3\n",
        "  K_SHOT = 3\n",
        "  sampler = FewShotBatchSampler(torch.as_tensor(targets_idx),N_WAY, K_SHOT, include_query= False, shuffle=True, shuffle_once=True)\n",
        "\n",
        "  def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = [item.t() for item in batch]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "    return batch.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "  def collate_fn(batch):\n",
        "          tensors, targets = [], []\n",
        "          for waveform, label in batch:\n",
        "                  tensors += [torch.squeeze(waveform)]\n",
        "                  targets += [label_to_index(labels, label)]\n",
        "                  \n",
        "          tensors = torch.unsqueeze(pad_sequence(tensors), 1)\n",
        "          targets = torch.stack(targets)\n",
        "          return tensors, targets\n",
        "\n",
        "  train_data_loader = DataLoader(novel_train_set, batch_sampler=sampler, collate_fn=collate_fn)\n",
        "\n",
        "  #memory required to iterate through training data\n",
        "  training_data = next(iter(train_data_loader))\n"
      ],
      "metadata": {
        "id": "UuLT_vtBVqPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bcf044-16a2-4c77-e695-f0ddfd36b68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mprun_meta_data_processing_1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from mprun_meta_data_processing import data_processing\n",
        "%mprun -f data_processing data_processing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdx7XD23Qoqj",
        "outputId": "442eba70-1388-4d8a-bc4e-e274327e5c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CPU times: user 26.7 s, sys: 556 ms, total: 27.3 s\n",
            "Wall time: 28.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%memit data_processing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVDJZ3TZTajl",
        "outputId": "a88c3657-f7e5-4f04-97ef-29653835965f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 436.70 MiB, increment: 0.20 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ADxDdzg7URTu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
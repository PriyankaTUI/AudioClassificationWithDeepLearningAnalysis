{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_snipet_metadataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2c8J1RxyoPHFZxYNUy+Z8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2bff9727f114938924f1cd4991ce3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_395502346a744fe090f8d8c207ef0c37",
              "IPY_MODEL_2bf776d965254fd2a34757e52aa3f570",
              "IPY_MODEL_e6c1ef77943f40b1a2dadff405c3c95c"
            ],
            "layout": "IPY_MODEL_271072ef87204bd88f0ac6bec8220b3b"
          }
        },
        "395502346a744fe090f8d8c207ef0c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad0e762c087f4437bfab955f32e5cad1",
            "placeholder": "​",
            "style": "IPY_MODEL_3be19d83c4cd4fd6ab6079aaa5fe931f",
            "value": "100%"
          }
        },
        "2bf776d965254fd2a34757e52aa3f570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f146f495852342fb8820b44ff6e100ec",
            "max": 2428923189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2845f2909c7b4447aef4f6ada643f7d7",
            "value": 2428923189
          }
        },
        "e6c1ef77943f40b1a2dadff405c3c95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2f0d32dcd5c4841a6a51c7b3c24e8e6",
            "placeholder": "​",
            "style": "IPY_MODEL_bbe7e7e895a544459a86fd074c81ec36",
            "value": " 2.26G/2.26G [01:15&lt;00:00, 34.9MB/s]"
          }
        },
        "271072ef87204bd88f0ac6bec8220b3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad0e762c087f4437bfab955f32e5cad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3be19d83c4cd4fd6ab6079aaa5fe931f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f146f495852342fb8820b44ff6e100ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2845f2909c7b4447aef4f6ada643f7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2f0d32dcd5c4841a6a51c7b3c24e8e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbe7e7e895a544459a86fd074c81ec36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1eb30d321c642af80aa363697b7c383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4ee98b973a54c6790c915acf303ef03",
              "IPY_MODEL_dd2a834bfed84ecbbe3e3288dc69a190",
              "IPY_MODEL_58d194fbd24d4c47839900c7fb19c953"
            ],
            "layout": "IPY_MODEL_a2881216712b4acb86eacfefaa8c2a64"
          }
        },
        "f4ee98b973a54c6790c915acf303ef03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54d48771454d4627a3cd39d460184675",
            "placeholder": "​",
            "style": "IPY_MODEL_33c63ee081534670b0be1a9892d6cca1",
            "value": "100%"
          }
        },
        "dd2a834bfed84ecbbe3e3288dc69a190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5bc453dc5474a4784739ea3ae438052",
            "max": 2428923189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd46c459169b41789ce174b8d853162c",
            "value": 2428923189
          }
        },
        "58d194fbd24d4c47839900c7fb19c953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20100f88d364342873231d2d028856e",
            "placeholder": "​",
            "style": "IPY_MODEL_fd0520a537f740648279e59abb084d43",
            "value": " 2.26G/2.26G [01:08&lt;00:00, 34.4MB/s]"
          }
        },
        "a2881216712b4acb86eacfefaa8c2a64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54d48771454d4627a3cd39d460184675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33c63ee081534670b0be1a9892d6cca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5bc453dc5474a4784739ea3ae438052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd46c459169b41789ce174b8d853162c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c20100f88d364342873231d2d028856e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd0520a537f740648279e59abb084d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyankaTUI/AudioClassificationWithDeepLearningAnalysis/blob/master/dataset/data_processing_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PriyankaTUI/AudioClassificationWithDeepLearningAnalysis.git\n",
        "%cd AudioClassificationWithDeepLearningAnalysis/dataset\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUQmWZCkw_vZ",
        "outputId": "1980309e-aa20-4c77-b842-9b201eb07530"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AudioClassificationWithDeepLearningAnalysis'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 215 (delta 52), reused 29 (delta 23), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (215/215), 44.37 MiB | 29.64 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n",
            "/content/AudioClassificationWithDeepLearningAnalysis/dataset\n",
            "/content/AudioClassificationWithDeepLearningAnalysis/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **For pre-data processing, we use two different strategies. We will evaluate both strategies below based on time management and memory management.**\n",
        "\n",
        "\n",
        "Using the memory profiler module, we will compute memory allocation at each stage of data processing to determine the amount of memory that is required. Additionally, we'll utilize the %%time command to track the overall processing time for data.\n",
        "\n",
        "Please check the results from cells 7 and 10. It provides us with thorough details on memory profiling for both methods. Based on those results, we may decide which technique is more efficient.\n",
        "\n"
      ],
      "metadata": {
        "id": "IIpmdBqrwf9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlvOCyXsMICE",
        "outputId": "ae457fdc-c6de-4208-8126-f661f671fe71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.7/dist-packages (0.60.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "id": "0TYvpIU-Rjjw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Fist data processing approach\n",
        "\n",
        "\n",
        "**With this strategy, all data will be processed beforehand and stored locally for use during training.**\n"
      ],
      "metadata": {
        "id": "sQbll3kAxNUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file mprun_data_processing.py\n",
        "\n",
        "\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "import torchaudio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def label_to_index(labels, label):\n",
        "    # Return the position of the word in labels\n",
        "    return torch.tensor(labels.index(label))\n",
        "\n",
        "def index_to_label(labels, index):\n",
        "    # Return the word corresponding to the index in labels\n",
        "    # This is the inverse of label_to_index\n",
        "    return labels[index]\n",
        "\n",
        "    \n",
        "def load_and_preprocess_speech_command_dataset(random_targets, digits):\n",
        "    random_targets = ['right', 'down', 'yes', 'sheila', 'marvin', 'backward', 'follow', 'bed', 'bird', 'cat', 'dog', 'happy', 'left', 'stop']\n",
        "    digits = ['zero','one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
        "    tensors = []\n",
        "    targets = []\n",
        "\n",
        "    #old classes storage\n",
        "    old_class_tensors = []\n",
        "    old_class_targets = []\n",
        "    \n",
        "    labels = digits + random_targets\n",
        "\n",
        "    ### saving local file for some data to save time for creating and processing new data\n",
        "    ### while continuously working on project \n",
        "    ### we can delete local files and always create new data\n",
        "    if (os.path.exists(path='data/novel_class_tensors.pt') and \n",
        "        os.path.exists(path='data/novel_class_targets.pt') and \n",
        "        os.path.exists(path='data/old_class_tensors.pt') and \n",
        "        os.path.exists(path='data/old_class_targets.pt')):\n",
        "        \n",
        "        tensors = torch.load('data/novel_class_tensors.pt')\n",
        "        targets = torch.load('data/novel_class_targets.pt')\n",
        "        old_class_tensors = torch.load('data/old_class_tensors.pt')\n",
        "        old_class_targets = torch.load('data/old_class_targets.pt')\n",
        "\n",
        "    else:\n",
        "\n",
        "\n",
        "        #  Loading dataset and custom dataloader\n",
        "        dataset = torchaudio.datasets.SPEECHCOMMANDS('./data/' , url = 'speech_commands_v0.02', folder_in_archive= 'SpeechCommands',  download = True)\n",
        "        #parameters for MFCC transformation\n",
        "        n_fft = 2048\n",
        "        win_length = None\n",
        "        hop_length = 512\n",
        "        n_mels = 256\n",
        "        n_mfcc = 256\n",
        "\n",
        "        for waveform, sample_rate, label, *_ in dataset:\n",
        "            if label in random_targets:\n",
        "                if sample_rate == 16000:\n",
        "                    if waveform.shape == (1, 16000):\n",
        "                        tensors += [torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=32, \n",
        "                                                                melkwargs={\n",
        "                                                                            'n_fft': n_fft,\n",
        "                                                                            'n_mels': n_mels,\n",
        "                                                                            'hop_length': hop_length,\n",
        "                                                                            'mel_scale': 'htk',\n",
        "                                                                            }\n",
        "                                                                            )(waveform)]\n",
        "                        targets += [label_to_index(labels, label)]\n",
        "\n",
        "            if label in digits:\n",
        "                if sample_rate == 16000:\n",
        "                    if waveform.shape == (1, 16000):\n",
        "                        old_class_tensors += [torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=32, \n",
        "                                                                melkwargs={\n",
        "                                                                            'n_fft': n_fft,\n",
        "                                                                            'n_mels': n_mels,\n",
        "                                                                            'hop_length': hop_length,\n",
        "                                                                            'mel_scale': 'htk',\n",
        "                                                                            }\n",
        "                                                                            )(waveform)]\n",
        "                        old_class_targets += [label_to_index(labels, label)]\n",
        "\n",
        "    torch.save(tensors, 'data/novel_class_tensors.pt')\n",
        "    torch.save(targets, 'data/novel_class_targets.pt')\n",
        "    torch.save(old_class_tensors, 'data/old_class_tensors.pt')\n",
        "    torch.save(old_class_targets, 'data/old_class_targets.pt')\n",
        "        \n",
        "    return {\"tensors\": tensors, \"targets\": targets,\n",
        "            \"old_class_tensors\": old_class_tensors, \n",
        "            \"old_class_targets\": old_class_targets}\n",
        "    \n",
        "\n",
        "\n",
        "class SpeechCommandSubDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,data,labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)    \n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        # print(f\"getting data {idx}\")\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "######starting processing##########\n",
        "def data_preprocessing():\n",
        "  random_targets = ['right', 'down', 'yes', 'sheila', 'marvin', 'backward', 'follow', 'bed', 'bird', 'cat', 'dog', 'happy', 'left', 'stop']\n",
        "  digits = ['zero','one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
        "  data = load_and_preprocess_speech_command_dataset(random_targets=random_targets, digits=digits)\n",
        "  tensors = data[\"tensors\"]\n",
        "  targets = data[\"targets\"]\n",
        "  old_class_tensors = data[\"old_class_tensors\"]\n",
        "  old_class_targets = data[\"old_class_targets\"]\n",
        "  \n",
        "  #taking small amount of sample data for training and testing\n",
        "  random_index = np.random.randint(len(tensors), size=500)\n",
        "  # print(f\"Random index: {random_index}\")\n",
        "  valid_dataset = SpeechCommandSubDataset(data = [tensors[index] for index in random_index], \n",
        "                                          labels = [targets[index] for index in random_index])\n",
        "\n",
        "  traindata, testdata = random_split(valid_dataset, [round(len(valid_dataset)*.6), round(len(valid_dataset)*.4)])\n",
        "  trainloader = DataLoader(traindata, batch_size=10, shuffle=True)\n",
        "  testloader = DataLoader(testdata, batch_size=10, shuffle=True)\n",
        "\n",
        "  # #creating old data loder for measures\n",
        "  random_index_olddata = np.random.randint(len(old_class_tensors), size=200)\n",
        "  # print(f\"Random index: {random_index}\")\n",
        "  old_class_dataset = SpeechCommandSubDataset(data = [old_class_tensors[index] for index in random_index_olddata], \n",
        "                                          labels = [old_class_targets[index] for index in random_index_olddata])\n",
        "\n",
        "  old_class_testloader = DataLoader(old_class_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "  #to check time required for iterating training data\n",
        "  for i, (input, lables) in enumerate(traindata):\n",
        "    print(\"\", end= \"\")\n",
        "  \n",
        "  #to check time required for iterating testing data\n",
        "  for i, (input, lables) in enumerate(testloader):\n",
        "    print(\"\", end= \"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyTN4-hUxmbX",
        "outputId": "d98e2def-f8f3-45a8-b300-20b863b3a276"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mprun_data_processing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from mprun_data_processing import data_preprocessing\n",
        "%mprun -f data_preprocessing data_preprocessing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "a2bff9727f114938924f1cd4991ce3c0",
            "395502346a744fe090f8d8c207ef0c37",
            "2bf776d965254fd2a34757e52aa3f570",
            "e6c1ef77943f40b1a2dadff405c3c95c",
            "271072ef87204bd88f0ac6bec8220b3b",
            "ad0e762c087f4437bfab955f32e5cad1",
            "3be19d83c4cd4fd6ab6079aaa5fe931f",
            "f146f495852342fb8820b44ff6e100ec",
            "2845f2909c7b4447aef4f6ada643f7d7",
            "f2f0d32dcd5c4841a6a51c7b3c24e8e6",
            "bbe7e7e895a544459a86fd074c81ec36"
          ]
        },
        "id": "VN3HeElSSfPA",
        "outputId": "e1e63cd0-1d89-4a5f-885d-f750c0da70f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/memory_profiler.py\", line 845, in enable\n",
            "    sys.settrace(self.trace_memory_usage)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/2.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2bff9727f114938924f1cd4991ce3c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/memory_profiler.py\", line 848, in disable\n",
            "    sys.settrace(self._original_trace_function)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CPU times: user 14min 12s, sys: 1min 4s, total: 15min 16s\n",
            "Wall time: 15min 23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Filename: /content/AudioClassificationWithDeepLearningAnalysis/dataset/mprun_data_processing.py\n",
        "\n",
        "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
        "=============================================================\n",
        "   112    268.4 MiB    268.4 MiB           1   def data_preprocessing():\n",
        "   113    268.4 MiB      0.0 MiB           1     random_targets = ['right', 'down', 'yes', 'sheila', 'marvin', 'backward', 'follow', 'bed', 'bird', 'cat', 'dog', 'happy', 'left', 'stop']\n",
        "   114    268.4 MiB      0.0 MiB           1     digits = ['zero','one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
        "   115    748.2 MiB    479.9 MiB           1     data = load_and_preprocess_speech_command_dataset(random_targets=random_targets, digits=digits)\n",
        "   116    748.2 MiB      0.0 MiB           1     tensors = data[\"tensors\"]\n",
        "   117    748.2 MiB      0.0 MiB           1     targets = data[\"targets\"]\n",
        "   118    748.2 MiB      0.0 MiB           1     old_class_tensors = data[\"old_class_tensors\"]\n",
        "   119    748.2 MiB      0.0 MiB           1     old_class_targets = data[\"old_class_targets\"]\n",
        "   120                                           \n",
        "   121                                           #taking small amount of sample data for training and testing\n",
        "   122    748.2 MiB      0.0 MiB           1     random_index = np.random.randint(len(tensors), size=500)\n",
        "   123                                           # print(f\"Random index: {random_index}\")\n",
        "   124    748.2 MiB      0.0 MiB         503     valid_dataset = SpeechCommandSubDataset(data = [tensors[index] for index in random_index], \n",
        "   125    748.2 MiB      0.0 MiB         503                                             labels = [targets[index] for index in random_index])\n",
        "   126                                         \n",
        "   127    748.2 MiB      0.0 MiB           1     traindata, testdata = random_split(valid_dataset, [round(len(valid_dataset)*.6), round(len(valid_dataset)*.4)])\n",
        "   128    748.2 MiB      0.0 MiB           1     trainloader = DataLoader(traindata, batch_size=10, shuffle=True)\n",
        "   129    748.2 MiB      0.0 MiB           1     testloader = DataLoader(testdata, batch_size=10, shuffle=True)\n",
        "   130                                         \n",
        "   131                                           # #creating old data loder for measures\n",
        "   132    748.2 MiB      0.0 MiB           1     random_index_olddata = np.random.randint(len(old_class_tensors), size=200)\n",
        "   133                                           # print(f\"Random index: {random_index}\")\n",
        "   134    748.2 MiB      0.0 MiB         203     old_class_dataset = SpeechCommandSubDataset(data = [old_class_tensors[index] for index in random_index_olddata], \n",
        "   135    748.2 MiB      0.0 MiB         203                                             labels = [old_class_targets[index] for index in random_index_olddata])\n",
        "   136                                         \n",
        "   137    748.2 MiB      0.0 MiB           1     old_class_testloader = DataLoader(old_class_dataset, batch_size=10, shuffle=True)\n",
        "   138                                         \n",
        "   139                                           #to check time required for iterating training data\n",
        "   140    748.2 MiB      0.0 MiB         301     for i, (input, lables) in enumerate(traindata):\n",
        "   141    748.2 MiB      0.0 MiB         300       print(\"\", end= \"\")\n",
        "   142                                           \n",
        "   143                                           #to check time required for iterating testing data\n",
        "   144    748.6 MiB      0.4 MiB          21     for i, (input, lables) in enumerate(testloader):\n",
        "   145    748.6 MiB      0.0 MiB          20       print(\"\", end= \"\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cAVxDKTRJsuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data/SpeechCommands"
      ],
      "metadata": {
        "id": "tE9j55uyDzRc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second data processing approach\n",
        "\n",
        "**With this strategy, all data will be processed in form of list and processed only during training.**"
      ],
      "metadata": {
        "id": "W_jO00dAxojM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file mprun_meta_data_processing.py\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "import torchaudio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def label_to_index(labels, label):\n",
        "    # Return the position of the word in labels\n",
        "    return torch.tensor(labels.index(label))\n",
        "\n",
        "def index_to_label(labels, index):\n",
        "    # Return the word corresponding to the index in labels\n",
        "    # This is the inverse of label_to_index\n",
        "    return labels[index]\n",
        "\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None, subset_type : str = None):\n",
        "        digits = ['zero','one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
        "        super().__init__(\"./\", download=True)\n",
        "        n_fft = 2048\n",
        "        win_length = None\n",
        "        hop_length = 512\n",
        "        n_mels = 256\n",
        "        n_mfcc = 256\n",
        "        sampling_rate = 16000\n",
        "        self.transform = torchaudio.transforms.MFCC(sample_rate=sampling_rate, n_mfcc=32, \n",
        "                                                                        melkwargs={\n",
        "                                                                                    'n_fft': n_fft,\n",
        "                                                                                    'n_mels': n_mels,\n",
        "                                                                                    'hop_length': hop_length,\n",
        "                                                                                    'mel_scale': 'htk',\n",
        "                                                                                    }\n",
        "                                                                                    )\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as fileobj:\n",
        "                if subset_type == 'old':\n",
        "                    return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj if line.startswith(tuple(digits))]\n",
        "                elif subset_type == 'novel':\n",
        "                    return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj if not line.startswith(tuple(digits))]\n",
        "                else:\n",
        "                    return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
        "\n",
        "        if subset == \"training\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "\n",
        "            \n",
        "    def __getitem__(self,idx):\n",
        "        waveform, sampling_rate, label, *_ = super().__getitem__(idx)\n",
        "        #returning waveform and it's label\n",
        "        if self.transform is not None:\n",
        "            waveform = self.transform(waveform)\n",
        "        return waveform, label\n",
        "\n",
        "class FewShotBatchSampler(object):\n",
        "\n",
        "    def __init__(self, dataset_targets, N_way, K_shot, include_query=False, shuffle=True, shuffle_once=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            dataset_targets - PyTorch tensor of the labels of the data elements.\n",
        "            N_way - Number of classes to sample per batch.\n",
        "            K_shot - Number of examples to sample per class in the batch.\n",
        "            include_query - If True, returns batch of size N_way*K_shot*2, which\n",
        "                            can be split into support and query set. Simplifies\n",
        "                            the implementation of sampling the same classes but\n",
        "                            distinct examples for support and query set.\n",
        "            shuffle - If True, examples and classes are newly shuffled in each\n",
        "                      iteration (for training)\n",
        "            shuffle_once - If True, examples and classes are shuffled once in\n",
        "                           the beginning, but kept constant across iterations\n",
        "                           (for validation)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dataset_targets = dataset_targets\n",
        "        self.N_way = N_way\n",
        "        self.K_shot = K_shot\n",
        "        self.shuffle = shuffle\n",
        "        self.include_query = include_query\n",
        "        if self.include_query:\n",
        "            self.K_shot *= 2\n",
        "        self.batch_size = self.N_way * self.K_shot  # Number of overall images per batch\n",
        "\n",
        "        # Organize examples by class\n",
        "        self.classes = torch.unique(self.dataset_targets).tolist()\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.indices_per_class = {}\n",
        "        self.batches_per_class = {}  # Number of K-shot batches that each class can provide\n",
        "        for c in self.classes:\n",
        "            self.indices_per_class[c] = torch.where(self.dataset_targets == c)[0]\n",
        "            self.batches_per_class[c] = self.indices_per_class[c].shape[0] // self.K_shot\n",
        "\n",
        "        # Create a list of classes from which we select the N classes per batch\n",
        "        self.iterations = sum(self.batches_per_class.values()) // self.N_way\n",
        "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
        "        if shuffle_once or self.shuffle:\n",
        "            self.shuffle_data()\n",
        "        else:\n",
        "            # For testing, we iterate over classes instead of shuffling them\n",
        "            sort_idxs = [i+p*self.num_classes for i,\n",
        "                         c in enumerate(self.classes) for p in range(self.batches_per_class[c])]\n",
        "            self.class_list = np.array(self.class_list)[np.argsort(sort_idxs)].tolist()\n",
        "\n",
        "    def shuffle_data(self):\n",
        "        # Shuffle the examples per class\n",
        "        for c in self.classes:\n",
        "            perm = torch.randperm(self.indices_per_class[c].shape[0])\n",
        "            self.indices_per_class[c] = self.indices_per_class[c][perm]\n",
        "        # Shuffle the class list from which we sample. Note that this way of shuffling\n",
        "        # does not prevent to choose the same class twice in a batch. However, for\n",
        "        # training and validation, this is not a problem.\n",
        "        random.shuffle(self.class_list)\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Shuffle data\n",
        "        if self.shuffle:\n",
        "            self.shuffle_data()\n",
        "\n",
        "        # Sample few-shot batches\n",
        "        start_index = defaultdict(int)\n",
        "        for it in range(self.iterations):\n",
        "            class_batch = self.class_list[it*self.N_way:(it+1)*self.N_way]  # Select N classes for the batch\n",
        "            index_batch = []\n",
        "            for c in class_batch:  # For each class, select the next K examples and add them to the batch\n",
        "                index_batch.extend(self.indices_per_class[c][start_index[c]:start_index[c]+self.K_shot])\n",
        "                start_index[c] += self.K_shot\n",
        "            if self.include_query:  # If we return support+query set, sort them so that they are easy to split\n",
        "                index_batch = index_batch[::2] + index_batch[1::2]\n",
        "            yield index_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.iterations\n",
        "\n",
        "\n",
        "def data_processing():\n",
        "  # from torch.utils.data.dataset import Subset\n",
        "  old_train_set = SubsetSC(\"training\", \"old\")\n",
        "  old_test_set = SubsetSC(\"testing\", \"old\")\n",
        "  novel_train_set = SubsetSC(\"training\", \"novel\")\n",
        "  novel_test_set = SubsetSC(\"testing\", \"novel\")\n",
        "\n",
        "  training_data = next(iter(novel_train_set))\n",
        "\n",
        "  #labels would be combination of novel classes and old classes(digits)\n",
        "  targets_list = [os.path.basename(os.path.dirname(novel_train_set._walker[i])) for i in range(len(novel_train_set))]\n",
        "  targets = list(set(targets_list))\n",
        "  digits = ['zero','one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
        "  labels = digits + targets \n",
        "  targets_idx = [label_to_index(targets, i) for i in targets_list]\n",
        "  # targets_idx = [list(targets).index(i) for i in targets_list]\n",
        "  N_WAY = 3\n",
        "  K_SHOT = 3\n",
        "  sampler = FewShotBatchSampler(torch.as_tensor(targets_idx),N_WAY, K_SHOT, include_query= False, shuffle=True, shuffle_once=True)\n",
        "\n",
        "  def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = [item.t() for item in batch]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "    return batch.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "  def collate_fn(batch):\n",
        "          tensors, targets = [], []\n",
        "          for waveform, label in batch:\n",
        "                  tensors += [torch.squeeze(waveform)]\n",
        "                  targets += [label_to_index(labels, label)]\n",
        "                  \n",
        "          tensors = torch.unsqueeze(pad_sequence(tensors), 1)\n",
        "          targets = torch.stack(targets)\n",
        "          return tensors, targets\n",
        "\n",
        "  train_data_loader = DataLoader(novel_train_set, batch_sampler=sampler, collate_fn=collate_fn)\n",
        "\n",
        "  #memory required to iterate through training data\n",
        "  training_data = next(iter(train_data_loader))\n"
      ],
      "metadata": {
        "id": "UuLT_vtBVqPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d8d607-db7c-4ab7-bc44-b2dbd8203c9c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mprun_meta_data_processing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from mprun_meta_data_processing import data_processing\n",
        "%mprun -f data_processing data_processing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "d1eb30d321c642af80aa363697b7c383",
            "f4ee98b973a54c6790c915acf303ef03",
            "dd2a834bfed84ecbbe3e3288dc69a190",
            "58d194fbd24d4c47839900c7fb19c953",
            "a2881216712b4acb86eacfefaa8c2a64",
            "54d48771454d4627a3cd39d460184675",
            "33c63ee081534670b0be1a9892d6cca1",
            "b5bc453dc5474a4784739ea3ae438052",
            "dd46c459169b41789ce174b8d853162c",
            "c20100f88d364342873231d2d028856e",
            "fd0520a537f740648279e59abb084d43"
          ]
        },
        "id": "gdx7XD23Qoqj",
        "outputId": "9dc0ec71-25ad-4bb8-ee25-1154ebdbd1ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/2.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1eb30d321c642af80aa363697b7c383"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CPU times: user 3min 35s, sys: 36.7 s, total: 4min 12s\n",
            "Wall time: 4min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Filename: /content/AudioClassificationWithDeepLearningAnalysis/dataset/mprun_meta_data_processing.py\n",
        "\n",
        "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
        "=============================================================\n",
        "   143    747.1 MiB    747.1 MiB           1   def data_processing():\n",
        "   144                                           # from torch.utils.data.dataset import Subset\n",
        "   145    351.4 MiB   -395.6 MiB           1     old_train_set = SubsetSC(\"training\", \"old\")\n",
        "   146    352.4 MiB      1.0 MiB           1     old_test_set = SubsetSC(\"testing\", \"old\")\n",
        "   147    353.4 MiB      1.0 MiB           1     novel_train_set = SubsetSC(\"training\", \"novel\")\n",
        "   148    354.4 MiB      1.0 MiB           1     novel_test_set = SubsetSC(\"testing\", \"novel\")\n",
        "   149                                         \n",
        "   150    355.3 MiB      0.9 MiB           1     training_data = next(iter(novel_train_set))\n",
        "   151                                         \n",
        "   152                                           #labels would be combination of novel classes and old classes(digits)\n",
        "   153    355.3 MiB      0.0 MiB        6341     targets_list = [os.path.basename(os.path.dirname(novel_train_set._walker[i])) for i in range(len(novel_train_set))]\n",
        "   154    355.3 MiB      0.0 MiB           1     targets = list(set(targets_list))\n",
        "   155    355.3 MiB      0.0 MiB           1     digits = ['zero','one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'] \n",
        "   156    355.3 MiB      0.0 MiB           1     labels = digits + targets \n",
        "   157    355.6 MiB      0.3 MiB        6341     targets_idx = [label_to_index(targets, i) for i in targets_list]\n",
        "   158                                           # targets_idx = [list(targets).index(i) for i in targets_list]\n",
        "   159    355.6 MiB      0.0 MiB           1     N_WAY = 3\n",
        "   160    355.6 MiB      0.0 MiB           1     K_SHOT = 3\n",
        "   161    356.3 MiB      0.7 MiB           1     sampler = FewShotBatchSampler(torch.as_tensor(targets_idx),N_WAY, K_SHOT, include_query= False, shuffle=True, shuffle_once=True)\n",
        "   162                                         \n",
        "   163    358.0 MiB      0.0 MiB           2     def pad_sequence(batch):\n",
        "   164                                             # Make all tensor in a batch the same length by padding with zeros\n",
        "   165    358.2 MiB      0.2 MiB          12       batch = [item.t() for item in batch]\n",
        "   166    358.2 MiB      0.0 MiB           1       batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "   167    358.2 MiB      0.0 MiB           1       return batch.permute(0, 2, 1)\n",
        "   168                                         \n",
        "   169                                         \n",
        "   170    358.0 MiB      1.7 MiB           2     def collate_fn(batch):\n",
        "   171    358.0 MiB      0.0 MiB           1             tensors, targets = [], []\n",
        "   172    358.0 MiB      0.0 MiB          10             for waveform, label in batch:\n",
        "   173    358.0 MiB      0.0 MiB           9                     tensors += [torch.squeeze(waveform)]\n",
        "   174    358.0 MiB      0.0 MiB           9                     targets += [label_to_index(labels, label)]\n",
        "   175                                                           \n",
        "   176    358.2 MiB      0.0 MiB           1             tensors = torch.unsqueeze(pad_sequence(tensors), 1)\n",
        "   177    358.2 MiB      0.0 MiB           1             targets = torch.stack(targets)\n",
        "   178    358.2 MiB      0.0 MiB           1             return tensors, targets\n",
        "   179                                         \n",
        "   180    356.3 MiB      0.0 MiB           1     train_data_loader = DataLoader(novel_train_set, batch_sampler=sampler, collate_fn=collate_fn)\n",
        "   181                                         \n",
        "   182                                           #memory required to iterate through training data\n",
        "   183    358.2 MiB      0.0 MiB           1     training_data = next(iter(train_data_loader))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4vpB7vhFL8Ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The processing of all the data using the first strategy takes about 15 minutes, while processing the list using the second way only takes about 3 minutes. Furthermore, the first strategy uses 748.6 MiB of memory, which is significantly higher than the second approach's 358.2 MiB of memory. Therefore, using the second strategy of processing data as needed rather than the first approach, which processes all data processing in advance, is more efficient."
      ],
      "metadata": {
        "id": "fPdYnTm3MbRi"
      }
    }
  ]
}